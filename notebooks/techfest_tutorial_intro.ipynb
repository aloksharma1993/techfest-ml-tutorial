{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%html is a cell magic, but the cell body is empty."
     ]
    }
   ],
   "source": [
    "# %%html \"static/foundation-5.5.1.css\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# ML Tutorial by\n",
    "\n",
    "### Lydia Gu, David Kellogg, Arsen Mamikonyan\n",
    "\n",
    "Using scikit-learn and ipython notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## TODO list for this presentation\n",
    "\n",
    "- figure out how to display notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Machine Learning ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Machine learning is learning from data.\n",
    "- We have a lot of data and we use mathematics to learn something about the structure and patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Combine\n",
    "\n",
    "- (a lot of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- mathematics (and computational power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Machine learning explores the construction and study of algorithms that can learn from and make predictions on data.\n",
    "> \n",
    "> Ron Kohavi; Foster Provost (1998). \"Glossary of terms\". Machine Learning 30: 271–274."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some Machine Learning Examples #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Machine Learning example 1\n",
    "\n",
    "\n",
    "### Facebooks personalizes your newsfeed based on your likes and clicks\n",
    "\n",
    "<img class=\"small-10\" src=\"media/facebook_newsfeed.jpg\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Machine Learning example 2\n",
    "\n",
    "### Netflix shows you suggestions based on what you have watched and other netflix users have watched\n",
    "\n",
    "<img class=\"small-10\" src=\"media/netflix_recs.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Machine Learning example 3\n",
    "\n",
    "### Siri\n",
    "\n",
    "<img class=\"small-10\" src=\"media/siri.jpg\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised vs. Unsupervised Learning #\n",
    "\n",
    "#### Supervised Learning\n",
    "<img class=\"small-11 columns\" src=\"media/supervised learning.png\" />\n",
    "\n",
    "- We have data mapping a set of inputs to a set of outputs (the labels). We use this labelled data to develop a model that can take new unseen inputs and predict the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Unsupervised Learning\n",
    "<img class=\"small-11 columns\" src=\"media/unsupervised learning.png\" />\n",
    "\n",
    "- We have a set of data, but no labels. We want to find some hidden structure in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning #\n",
    "\n",
    "- Labeled data (to learn on)\n",
    "- Unlabeled data (to answer our question for)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Supervised Learning #\n",
    " In supervised learning, we typically have an outcome (for example, a stock price) that we want to predict from a set of input features (for example, previous stock price, company financial data). We use a set of input­outcome mappings to build a model that will predict the outcome on new, unseen input values.\n",
    " \n",
    "If the output is continuous, e.g. stock price, the problem is referred to as a regression problem. This is shown by the image on the right side of the figure below. The x­axis is a continuous input feature (ex. previous stock price), and the y­axis is some continuous output (ex. future stock price). The example fits a simple linear model to the data. If the output is categorical, e.g. is email or spam, the problem is referred to as a classification problem. This is shown by the image on the left side of the figure below. The x and y­axes are continuous input features (ex. age and education level), and the outcome is one of two classes (ex. employed/not employed). The model is a decision boundary, the dotted line, where on one side, the outcome is one class, and on the other, it is the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification vs Regression #\n",
    "\n",
    "#### Regression\n",
    "- labels are continuous\n",
    "    * what will be stock pice (tomorrow after closing)\n",
    "    * temperature in Phoenix (tomorrow at noon)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### Classification\n",
    "- labels are categorical\n",
    "    * Which will be the best talk at the Techfest?\n",
    "    * Will there be a good vegeterian option at lunch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "    <p class=\"small-6 columns\"> Regression </p>\n",
    "    <p class=\"small-6 columns\"> Classification </p>\n",
    "</div>\n",
    "<div class=\"row\">\n",
    "    <img class=\"small-6 columns\" src=\"media/regression.png\" />\n",
    "    <img class=\"small-6 columns\" src=\"media/classification.png\" />\n",
    "</div>\n",
    "\n",
    "- Citation: http://ipython­books.github.io/featured­04/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression\n",
    "\n",
    "- (In depth Regression example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classifiction\n",
    "\n",
    "- (In depth Classification example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning #\n",
    "\n",
    "We have\n",
    "\n",
    "- only unlabbeled data\n",
    "\n",
    "Goal\n",
    "\n",
    "- deduce some structure of the data and predict (is this true?) based on that data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Examples\n",
    "\n",
    "- clustering\n",
    "- density estimation\n",
    "- dimension reduction\n",
    "- principle components?\n",
    "- manifold learning (nonlinear dimension reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Unsupervised Learning #\n",
    "\n",
    "With unsupervised learning, we only have input features and no outcome data. The goal is instead to learn about the structure of the data. Here are examples of methods for unsupervised learning:\n",
    "- clustering: grouping the input data into a finite set of clusters, based on how similar the input features are. In the figure below, data points are grouped into 3 clusters based on the x and y­axis values. (TODO: better image)\n",
    "- density estimation: estimating a probability distribution for the data (why?)\n",
    "- dimension reduction: finding a lower­dimensional representation of the input data. This\n",
    "is often used for data visualization. What is this used for? compression?\n",
    "- principle components?\n",
    "- manifold learning (nonlinear dimension reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This tutorial #\n",
    "\n",
    "- We will spend most of the tutorial talking about supervised learning methods.\n",
    "- We will be using the Python library scikit-learn to work through some examples. Scikit-learn is a collection of useful machine learning tools, from the models themselves to tools for cleaning data and evaluating models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How is ML done\n",
    "\n",
    "<img class=\"small-11\" src=\"https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcT8TqGzN7vGVBzw2O5iAG-luBv92jL6ngtJJvf6VyJ8qyq98Aru\" />\n",
    "\n",
    "1. Get data\n",
    "2. Clean data\n",
    "3. Data Transformation (feature engineering)\n",
    "4. Fitting a model (Data Mining)\n",
    "5. Predicting\n",
    "6. Deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We will focus\n",
    "\n",
    "1. Get data\n",
    "2. Clean data\n",
    "3. **Data Transformation (feature engineering)**\n",
    "4. **Fit a model (Data Mining)**\n",
    "5. **Predicting**\n",
    "6. Deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Transformation (Feature Engineering) #\n",
    "\n",
    "<div class=\"row\">\n",
    "    <img class=\"small-5 columns\" src=\"media/gata.jpg\" />\n",
    "    <span class=\"small-2 small-centered columns\"> -> </span>\n",
    "    <img class=\"small-5 columns\" src=\"media/gata.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(Refer to example dataset)\n",
    "\n",
    "Your data is not usually a neatly organized matrix of values, such as what we’ve given you here. Data can be scattered in many areas, from database tables of business information to user activity logs. You need to extract features from your data, based on intuition about what would be predictive features. In addition, you may compute some function over the data, because the raw form is not amenable to use in a mathematical model. Some examples of feature extraction include:\n",
    "- in computer vision, computing SIFT features on each frame of a video sequence for autonomous navigation.\n",
    "- for facial recognition, computing eigenfaces from an image. This greatly reduces the dimension of the data.\n",
    "- computing aggregate statistics over user activity data (number of logins in past day, number of distinct page visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why can't we use original data?\n",
    "\n",
    "- It could be **scattered**. We need to **consolidate** the data\n",
    "- computer can't interpret the raw data. We have to guide it, transform the data into machine understandable format (construct features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What are features?\n",
    "\n",
    "- Each feature is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### What are features?\n",
    "\n",
    "- Each feature is **something**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Transformation Best Practices\n",
    "\n",
    "- ???(Arsen) I'm not sure if this section is helpful.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "After feature extraction, features often require some preprocessing to be properly used in numerical models:\n",
    "- data normalization: scaling data to between 0 and 1 or ­1 and 1. Features are normalized so that features with a wide range (ex. ­10000 to 10000) don’t skew the model to undervalue features with small ranges (ex. 0 to 0.1)\n",
    "- categorical inputs: How do we convert categorical inputs (for example, WebsiteBuilder account type: Personal, Business, Business Plus) to numerical values for use in a model? We can’t just assign increasing integers 0, 1 and 2 because this implies there is some order to the categories, which is typically not the case. Instead, we need to create N ­ 1 input features, where N is the number of categories in the categorical input. Each input features is a boolean, in this example is_personal and is_business. Only N ­ 1 features are needed because if all are false, then the value is Nth category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a model\n",
    "\n",
    "- A [mathematical] **Model** is the algorithm that we want to use to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To generalize data we break up the data into training and test sets\n",
    "- The training set is used to train the model, and the test set is used to evaluate the model performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## scikit-learn example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "clf = svm.SVC(gamma=0.001, C=100.)\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = clf.fit(digits.data[:-1], digits.target[:-1])\n",
    "clf.predict(digits.data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc68aab9a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD7CAYAAABZjGkWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDSREsIGECDaQ0P8B/jLlFrdmZv4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc68acc2090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation/Chosing a model\n",
    "\n",
    "- differnt types of models are designed to work better with different types of problems / different types of data\n",
    "- We want to \"simulate\" predicting with classifier as close as to a real world situation when evaluating the classifier\n",
    "- We randomly split training data into train/test and use test to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Code: Split dataset into train/test sets ­ show how to do this in\n",
    "scikit­learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Fitting a Model (alternate titles: Training a Model, Model Selection?)\n",
    "In supervised learning, a model is the ?? used to predict an outcome based on input features. One should consider model selection and model assessment.\n",
    "\n",
    "Different data is better described by different models. The goal is to select and train a model that best describes the data. But how do we know if a model describes the data well? We want a model that generalizes to unseen data. To estimate how well a model will generalize, we break up the data into training and test sets. The training set is used to train the model, and the test set is used to evaluate the model performance on unseen data.\n",
    "Let’s fit a simple [linear or logistic regression?] model to our dataset.\n",
    "Scikit­learn has a very simple API for fitting and evaluating a model. All estimators for supervised learning in the library implement the following API:\n",
    "- fit(X, y) ­ fits the model to the input features X and the corresponding outcomes y\n",
    "- predict(X) ­ predicts outcomes y on unseen input features X → Tool for splitting dataset into train/test? Or just example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "data_train, data_test, targets_train, targets_test = train_test_split(digits.data, digits.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-531b40699b0e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-531b40699b0e>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    iris_X_train = iris_X[indices[:‐10]]\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.permutation(len(iris_X))\n",
    "iris_X_train = iris_X[indices[:‐10]]\n",
    "iris_y_train = iris_y[indices[:‐10]]\n",
    "iris_X_test = iris_X[indices[‐10:]]\n",
    "iris_y_test = iris_y[indices[‐10:]]\n",
    "￼￼￼￼￼\n",
    "## There might be a function for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Overfitting, Underfitting and Bias vs. Variance\n",
    "When fitting a model, it is possible to tune the model too much to the training data, meaning that it will predict with low accuracy on the test data. This is referred to as overfitting. Conversely, the model can be tuned to predict the training data, as well as the test data, with low accuracy. This is referred to as underfitting. This problem of not over or underfitting is represented by the bias­variance tradeoff.\n",
    "Bias refers to error caused by picking an overly simple model that can’t fully fit the training data. Variance is the difference between the training set error and test set error. (From wikipedia http://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff):\n",
    "- The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "- The variance is error from sensitivity to small fluctuations in the training set. High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs.\n",
    "When selecting and training a model, how do we know if we have high bias or high variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assessing the Predictive Power of a Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification metrics\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- f1, roc auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression metrics\n",
    "With regression, outcomes are continuous, and different metrics are required to measure success. Common measures include the following:\n",
    "- mean squared error: average of the square of the difference between the predicted and the true outcome.\n",
    "- r2 score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting, Underfitting and Bias vs. Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Cross-validation\n",
    "Cross-validation is a technique for measuring this tradeoff. In cross-validation, the training data is split into two sets (names??), where one is used for training the model, and the other is used for evaluating the bias and variance.\n",
    "\n",
    "￼￼￼\n",
    "Explain the different way to split training and test sets (ex. Leave­One­Out)\n",
    "Reducing variance:\n",
    "dimensionality reduction, feature selection regularization\n",
    "larger training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Assessing the Predictive Power of a Model\n",
    "Code: Show how to evaluate the model in scikit learn\n",
    "How do we know if a model is “doing well”? In supervised learning, we have outcomes to compare predictions to, so we can develop quantitative measure of how well a model performs. In unsupervised learning, there are not outcomes to measure against, and how well a model performs is subjective. Here, we will focus on quantitative measures for supervised learning.\n",
    "With classification, each outcome is either predicted correctly or not, so the measure of success is binary. Let’s consider the case of binary classification: the outcome is either true or false. Common measures of success include the following:\n",
    "- accuracy: # of outcomes correctly predicted / total # of outcomes\n",
    ": # of True outcomes correctly predicted / # of predictions for True\n",
    ": # of True outcomes correctly predicted / # of True outcomes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
