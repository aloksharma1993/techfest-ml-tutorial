{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Classification\n",
    "\n",
    "In this tutorial, we will be working with **supervised learning** - learning a model from data with known outcomes. **Classification** is a form of supervised learning where the outcome is a discrete *class*. For example, if we wanted to build a spam detector for our email inbox, messages may be classified as *email* or *spam*. If we were building a model to recognize handwritten digits (for example to read checks at an ATM), the outcome would be one of 10 possible classes, one for each digit (0-9).\n",
    "\n",
    "Similarly, in a hospital, we may want to predict whether an Intensive Care Unit (ICU) patient will live or die. The [2012 PhysioNet/Computing in Cardiology challenge](http://physionet.org/challenge/2012/) provides a dataset of ICU stays longer than 48 hours and whether or not the patient died in the hospital. We can use this dataset to train a classifier that, based on the patient record, predicts if the patient will survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The raw dataset can be found on the challenge website linked above. However, for this tutorial, we did some preprocessing for you so that we can focus on the machine learning part. If you want to know more about what we did, see the [Data Preparation](#Data-Preparation) section at the bottom of the tutorial.\n",
    "\n",
    "We provide a [training set](https://raw.githubusercontent.com/lydiagu/ml-tutorial/master/physionet/train-a.csv) and a [test set](https://raw.githubusercontent.com/lydiagu/ml-tutorial/master/physionet/test-a.csv). Recall that a training set is used for training a model and a test set is used for evaluating the model performance on unseen data. The files are stored as .csv, with a header labeling the columns. Let's load the training set by using the `requests` library to download the file. `requests` is an easy-to-use library for making HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://raw.githubusercontent.com/lydiagu/ml-tutorial/master/physionet/train-a.csv')\n",
    "raw_data = response.content  # Read the .csv content from the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's [`csv`](https://docs.python.org/2/library/csv.html) library has a `reader` object that will parse the .csv file. You can create a `reader` object by calling `csv.reader()` and passing in a file as the argument. Because `csv.reader()` expects a file-like object as the argument, we wrap the downloaded data string in a [`StringIO`](https://docs.python.org/2/library/stringio.html) object, which allows you to read the string like a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import StringIO\n",
    "\n",
    "f = StringIO.StringIO(raw_data)\n",
    "reader = csv.reader(f)\n",
    "\n",
    "# The first row in the file is the header of column names.\n",
    "csv_header = reader.next()\n",
    "\n",
    "# Iterate over remaining rows to get the data.\n",
    "data = []\n",
    "for row in reader:\n",
    "    row = [float(r) for r in row]  # Convert all values to floats.\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's in our dataset? Let's print the .csv header to see the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALP_diff', 'ALP_first', 'ALP_last', 'ALP_max', 'ALP_mean', 'ALP_min', 'ALT_diff', 'ALT_first', 'ALT_last', 'ALT_max', 'ALT_mean', 'ALT_min', 'AST_diff', 'AST_first', 'AST_last', 'AST_max', 'AST_mean', 'AST_min', 'Age', 'Albumin_diff', 'Albumin_first', 'Albumin_last', 'Albumin_max', 'Albumin_mean', 'Albumin_min', 'BUN_diff', 'BUN_first', 'BUN_last', 'BUN_max', 'BUN_mean', 'BUN_min', 'Bilirubin_diff', 'Bilirubin_first', 'Bilirubin_last', 'Bilirubin_max', 'Bilirubin_mean', 'Bilirubin_min', 'Cholesterol_diff', 'Cholesterol_first', 'Cholesterol_last', 'Cholesterol_max', 'Cholesterol_mean', 'Cholesterol_min', 'Creatinine_diff', 'Creatinine_first', 'Creatinine_last', 'Creatinine_max', 'Creatinine_mean', 'Creatinine_min', 'DiasABP_diff', 'DiasABP_first', 'DiasABP_last', 'DiasABP_max', 'DiasABP_mean', 'DiasABP_min', 'FiO2_diff', 'FiO2_first', 'FiO2_last', 'FiO2_max', 'FiO2_mean', 'FiO2_min', 'GCS_diff', 'GCS_first', 'GCS_last', 'GCS_max', 'GCS_mean', 'GCS_min', 'Gender', 'Glucose_diff', 'Glucose_first', 'Glucose_last', 'Glucose_max', 'Glucose_mean', 'Glucose_min', 'HCO3_diff', 'HCO3_first', 'HCO3_last', 'HCO3_max', 'HCO3_mean', 'HCO3_min', 'HCT_diff', 'HCT_first', 'HCT_last', 'HCT_max', 'HCT_mean', 'HCT_min', 'HR_diff', 'HR_first', 'HR_last', 'HR_max', 'HR_mean', 'HR_min', 'Height', 'ICUType', 'K_diff', 'K_first', 'K_last', 'K_max', 'K_mean', 'K_min', 'Lactate_diff', 'Lactate_first', 'Lactate_last', 'Lactate_max', 'Lactate_mean', 'Lactate_min', 'MAP_diff', 'MAP_first', 'MAP_last', 'MAP_max', 'MAP_mean', 'MAP_min', 'MechVent_diff', 'MechVent_first', 'MechVent_last', 'MechVent_max', 'MechVent_mean', 'MechVent_min', 'Mg_diff', 'Mg_first', 'Mg_last', 'Mg_max', 'Mg_mean', 'Mg_min', 'NIDiasABP_diff', 'NIDiasABP_first', 'NIDiasABP_last', 'NIDiasABP_max', 'NIDiasABP_mean', 'NIDiasABP_min', 'NIMAP_diff', 'NIMAP_first', 'NIMAP_last', 'NIMAP_max', 'NIMAP_mean', 'NIMAP_min', 'NISysABP_diff', 'NISysABP_first', 'NISysABP_last', 'NISysABP_max', 'NISysABP_mean', 'NISysABP_min', 'Na_diff', 'Na_first', 'Na_last', 'Na_max', 'Na_mean', 'Na_min', 'PaCO2_diff', 'PaCO2_first', 'PaCO2_last', 'PaCO2_max', 'PaCO2_mean', 'PaCO2_min', 'PaO2_diff', 'PaO2_first', 'PaO2_last', 'PaO2_max', 'PaO2_mean', 'PaO2_min', 'Platelets_diff', 'Platelets_first', 'Platelets_last', 'Platelets_max', 'Platelets_mean', 'Platelets_min', 'RespRate_diff', 'RespRate_first', 'RespRate_last', 'RespRate_max', 'RespRate_mean', 'RespRate_min', 'SaO2_diff', 'SaO2_first', 'SaO2_last', 'SaO2_max', 'SaO2_mean', 'SaO2_min', 'SysABP_diff', 'SysABP_first', 'SysABP_last', 'SysABP_max', 'SysABP_mean', 'SysABP_min', 'Temp_diff', 'Temp_first', 'Temp_last', 'Temp_max', 'Temp_mean', 'Temp_min', 'TroponinI_diff', 'TroponinI_first', 'TroponinI_last', 'TroponinI_max', 'TroponinI_mean', 'TroponinI_min', 'TroponinT_diff', 'TroponinT_first', 'TroponinT_last', 'TroponinT_max', 'TroponinT_mean', 'TroponinT_min', 'Urine_diff', 'Urine_first', 'Urine_last', 'Urine_max', 'Urine_mean', 'Urine_min', 'WBC_diff', 'WBC_first', 'WBC_last', 'WBC_max', 'WBC_mean', 'WBC_min', 'Weight_diff', 'Weight_first', 'Weight_last', 'Weight_max', 'Weight_mean', 'Weight_min', 'pH_diff', 'pH_first', 'pH_last', 'pH_max', 'pH_mean', 'pH_min', 'In-hospital_death']\n"
     ]
    }
   ],
   "source": [
    "print csv_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, that's a lot of columns! Let's take a closer look. We refer to the input data as **features** and the output data (in-hospital death) as **labels**. The dataset has two kinds of features:\n",
    "  - **static features** that are measured at the begining of the ICU stay and don't change over time: age, gender, height, ICUType\n",
    "  - **time-varying features** that are measured and recorded at a specific point in time, and may be measured 0 or more times during the ICU stay: Weight, Glucose, etc.\n",
    "\n",
    "To have a uniform way of representing 0 or more records for each time-varying feature, we computed the min, max, mean, first value, last value and difference between first and last values for each. That's why there are so many features! The medical measurements themselves are explained in more detail on the [Physionet website](http://physionet.org/challenge/2012/).\n",
    "\n",
    "For time-varying features that were not recorded for a patient, the computed statistics have value -1. Similarly, static features that were not recorded are also represented by -1. -1 is a useful number for representing a missing value because all of the valid, non-missing values are nonnegative. \n",
    "\n",
    "The last field in the .csv file is the label: `In-hospital_death`. 1 means the patient died in the hospital. 0 means the patient survived.\n",
    "\n",
    "Let's separate the features from the label and store them in [`numpy`](http://docs.scipy.org/doc/numpy/user/index.html) arrays. `numpy` is a Python library useful for scientific computing, and it defines array objects for storing scientific data, as well as many mathematical functions. [`scikit-learn`](http://scikit-learn.org/stable/), the Python machine learning library we'll be using in this tutorial, is designed to work well with data in `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Python Tip:\n",
    "# In Python lists, an index of -1 represents the last value in the list. Python\n",
    "# list ranges do not include the interval end. So a range of [0:-1] does not\n",
    "# include the last value in the list.\n",
    "\n",
    "X = numpy.array([f[0:-1] for f in data])  # Features represented by `X`.\n",
    "y = numpy.array([f[-1] for f in data])  # Labels represented by `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data\n",
    "\n",
    "Now that we've loaded the data and looked at what features are in our dataset, let's dig into the data. A good place to start is by measuring the proportion of positive outcomes. The proportion of positive outcomes is just the mean of the labels, since we conveniently represented the labels as either 0 or 1. Since we put our data into numpy arrays, we can take advantage of some [very useful methods on arrays](http://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.145625\n"
     ]
    }
   ],
   "source": [
    "print y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion is fairly low (as would hopefully be expected with modern medicine!).\n",
    "\n",
    "Let's use a visualization library called [matplotlib](http://matplotlib.org/api/) to visualize some of the features. First, we'll look at one feature and plot the histogram to get an idea of the distribution of the values. `matplotlib` has a specific function [`hist()`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist) for computing and plotting histograms.\n",
    "\n",
    "The following line enables IPython Notebook's matplotlib mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE6VJREFUeJzt3X2MXfdd5/H3p0kNTcvWtViNH1EsiFc1Arbd1jwL75L1\nhqpr+6/ESCCrjfgnQFOkLbEjQfwXSyMBRVrlj4W2GiJi1lvAcrRd1ZO0owUhNS11IM3E2F7tQMat\nx31+AAG28t0/7hl8M7HnXs/Tvfbv/ZJGPud3Hu73Hnk+87vn/M49qSokSbe31426AEnS2jPsJakB\nhr0kNcCwl6QGGPaS1ADDXpIaMDDskxxN8mKSF5I8leQ7kmxKMpXkXJLTSTYuWv98krNJ9q1t+ZKk\nYWSpcfZJ7gY+Cby1qv4pyf8APg58P/Dlqno8ySPAW6rqSJLdwFPAO4FtwDPArqp6ZW3fhiRpKYN6\n9t8ErgB3JbkTuAv4ArAfmOzWmQQOdtMHgONVdaWqZoELwJ7VLlqSdHOWDPuq+irwW8Df0Qv5r1fV\nFDBRVfPdavPARDe9FZjr28UcvR6+JGmElgz7JN8LvB+4m16QvynJz/WvU73zQEt954LfxyBJI3bn\ngOXvAP6iqr4CkORPgB8FLiXZXFWXkmwBLnfrXwR29G2/vWt7lST+AZCkZaiqLGe7QWF/Fvi1JG8A\n/hG4F3gO+HvgMPDB7t+T3fqngKeS/Da90zf3dOuvWsG3myTHqurYqOsYBx6LazwW13gsrllJR3nJ\nsK+qv0ryB8BngVeAzwH/Hfgu4ESSB4FZ4P5u/ZkkJ4AZ4CrwUPm1mpI0coN69lTV48Dji5q/Sq+X\nf731fwP4jZWXJklaLd5BO3rToy5gjEyPuoAxMj3qAsbI9KgLuB0seVPVmr1oUp6zl6Sbs5LstGcv\nSQ0w7CWpAYa9JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJaoBhL0kNMOwlqQGGvSQ1YOC3Xkpq0zg9\nZMjv0lo5w17SEsYh78351eBpHElqgGEvSQ0w7CWpAYa9JDVgYNgn+TdJzvT9fCPJ+5JsSjKV5FyS\n00k29m1zNMn5JGeT7FvbtyBJGuSmHkuY5HXARWAP8MvAl6vq8SSPAG+pqiNJdgNPAe8EtgHPALuq\n6pW+/fhYQmnM9YZejsdoHPOiZz0fS3gvcKGqXgb2A5Nd+yRwsJs+AByvqitVNQtcoPfHQZI0Ijcb\n9oeA4930RFXNd9PzwEQ3vRWY69tmjl4PX5I0IkOHfZINwH8G/ufiZdU7F7TU571x+CwoSc26mTto\nfwb4y6r6Ujc/n2RzVV1KsgW43LVfBHb0bbe9a3uVJMf6ZqeravomapGk216SvcDeVdnXsBdok/wR\n8L+rarKbfxz4SlV9MMkRYOOiC7R7uHaB9vuq74W8QCuNPy/Qjp+VZOdQYZ/kjcDfAjur6ltd2ybg\nBPA9wCxwf1V9vVv2KPBe4CrwcFV9YrUKlrQ+DPvxs+Zhv9oMe2n8GfbjZz2HXkqSbkGGvSQ1wLCX\npAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgMMe0lq\ngGEvSQ0w7CWpAYa9JDXAsJekBgwV9kk2JvlYkpeSzCT54SSbkkwlOZfkdJKNfesfTXI+ydkk+9au\nfEnSMIbt2f8u8PGqeivwg8BZ4AgwVVW7gGe7eZLsBh4AdgP3AU8k8ROEJI3QwBBO8mbgJ6vqIwBV\ndbWqvgHsBya71SaBg930AeB4VV2pqlngArBntQuXJA1vmB73TuBLST6a5HNJfi/JG4GJqprv1pkH\nJrrprcBc3/ZzwLZVq1iSdNPuHHKdtwO/VFWfSfIhulM2C6qqktQS+3jNsiTH+manq2p6iFokqRlJ\n9gJ7V2Nfw4T9HDBXVZ/p5j8GHAUuJdlcVZeSbAEud8svAjv6tt/etb1KVR1bdtXSbW5A50mN6DrB\n0wvzSR5b7r4GnsapqkvAy0l2dU33Ai8CTwOHu7bDwMlu+hRwKMmGJDuBe4Dnllug1K4a8Y9uJ8P0\n7AF+GfjDJBuA/wu8B7gDOJHkQWAWuB+gqmaSnABmgKvAQ1Xl/xxJGqGMIoeTVFVl3V9YukX0TuOM\nuo8URl8DQDAvelaSnY5/l6QGGPaS1ADDXpIaYNhLUgMMe0lqwLBDLyVpZMbhJrNbfUSQYS/pFjDq\nrL+lcx7wNI4kNcGwl6QGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJekBhj2ktQA\nw16SGjBU2CeZTfLXSc4kea5r25RkKsm5JKeTbOxb/2iS80nOJtm3VsVLkoYzbM++gL1V9baq2tO1\nHQGmqmoX8Gw3T5LdwAPAbuA+4IkkfoKQpBG6mRBe/B2f+4HJbnoSONhNHwCOV9WVqpoFLgB7kCSN\nzM307J9J8tkkv9C1TVTVfDc9D0x001uBub5t54BtK65UkrRswz685Mer6otJ/jUwleRs/8KqqgFP\nknnNsiTH+manq2p6yFokqQlJ9gJ7V2NfQ4V9VX2x+/dLSf6U3mmZ+SSbq+pSki3A5W71i8COvs23\nd22L93lsJYVLa2UcHoEnAXSd4OmF+SSPLXdfA0/jJLkryXd1028E9gEvAKeAw91qh4GT3fQp4FCS\nDUl2AvcAzy23QGk0asQ/0uoapmc/AfxpkoX1/7CqTif5LHAiyYPALHA/QFXNJDkBzABXgYeqyv+9\nkjRCGUUOJ6lb/Untun31TuOMun8SrGHBONQRxiGzVpKdjn+XpAYY9pLUAMNekhpg2EtSAwx7SWqA\nYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJekBhj2\nktQAw16SGjBU2Ce5I8mZJE9385uSTCU5l+R0ko196x5Ncj7J2ST71qpwSdLwhu3ZPwzMcO2pv0eA\nqaraBTzbzZNkN/AAsBu4D3giiZ8eJGnEBgZxku3Au4Dfp/eYd4D9wGQ3PQkc7KYPAMer6kpVzQIX\ngD2rWbAk6eYN0+v+HeADwCt9bRNVNd9NzwMT3fRWYK5vvTlg20qLlCStzJ1LLUzybuByVZ1Jsvd6\n61RVJanrLVtY5Qb7PtY3O11V00uXKklt6XJ372rsa8mwB34M2J/kXcB3Av8qyZPAfJLNVXUpyRbg\ncrf+RWBH3/bbu7bXqKpjK6pckm5zXSd4emE+yWPL3deSp3Gq6tGq2lFVO4FDwCer6ueBU8DhbrXD\nwMlu+hRwKMmGJDuBe4DnllucJGl1DOrZL7ZwSuY3gRNJHgRmgfsBqmomyQl6I3euAg9V1VKneCRJ\n6yCjyOIkVVUZvKa0/nrXoEbdRwnWsGAc6gjjkFkryU7HwEtSAwx7SWqAYS9JDTDsJakBhr0kNcCw\nl6QGGPaS1ADDXpIaYNhLUgNu9usSpDUz4NtTJa2AYa8xMw55P/K74qVV52kcSWqAYS9JDTDsJakB\nhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgOWDPsk35nk00meTzKT5L927ZuSTCU5l+R0ko192xxNcj7J\n2ST71voNSJIGG/jA8SR3VdU/JLkT+HPgvwD7gS9X1eNJHgHeUlVHkuwGngLeCWwDngF2VdUri/bp\nA8f1GuPxoG8YlwdcW8OCcaijgQeOV9U/dJMbgDuAr9EL+8mufRI42E0fAI5X1ZWqmgUuAHuWU5gk\nafUMDPskr0vyPDAPfKqqXgQmqmq+W2UemOimtwJzfZvP0evhS5JGaOAXoXWnYP5tkjcDn0jy7xct\nrwHfVnjdZUmO9c1OV9X04HIlqR1J9gJ7V2NfQ3/rZVV9I8n/Av4dMJ9kc1VdSrIFuNytdhHY0bfZ\n9q7tevs7trySJakNXSd4emE+yWPL3deg0TjfvTDSJskbgP8InAFOAYe71Q4DJ7vpU8ChJBuS7ATu\nAZ5bbnGSpNUxqGe/BZhM8jp6fxierKpnk5wBTiR5EJgF7geoqpkkJ4AZ4CrwUA0a7iNJWnMDh16u\nyYs69FLX4dBLa7i+caijgaGXkqRbn2EvSQ0w7CWpAYa9JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJ\naoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBA8M+yY4kn0ry\nYpLPJ3lf174pyVSSc0lOJ9nYt83RJOeTnE2yby3fgCRpsIEPHE+yGdhcVc8neRPwl8BB4D3Al6vq\n8SSPAG+pqiNJdgNPAe8EtgHPALuq6pW+ffrAcb2GDxy3husbhzoaeOB4VV2qque76W8DL9EL8f3A\nZLfaJL0/AAAHgONVdaWqZoELwJ7lFCdJWh03dc4+yd3A24BPAxNVNd8tmgcmuumtwFzfZnP0/jhI\nkkbkzmFX7E7h/DHwcFV9K7n2SaKqqvcR/IZesyzJsb7Z6aqaHrYWSWpBkr3A3tXY11Bhn+T19IL+\nyao62TXPJ9lcVZeSbAEud+0XgR19m2/v2l6lqo4tu2pJakDXCZ5emE/y2HL3NcxonAAfBmaq6kN9\ni04Bh7vpw8DJvvZDSTYk2QncAzy33AIlSSs3zGicnwD+D/DXXDsdc5RegJ8AvgeYBe6vqq932zwK\nvBe4Su+0zycW7dPROHoNR+NYw/WNQx23/micgWG/Fgx7XY9hbw3XNw513Pph7x20ktQAw16SGmDY\nS1IDDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUgKG/9VK3twHfWirpFmfYq8+o837kd6NLty1P\n40hSAwx7SWqAp3EkaQi3+nUtw37EbvX/QFI7xuFXdfnXtQz7sXBr/yeSNP48Zy9JDTDsJakBwzxw\n/CNJ5pO80Ne2KclUknNJTifZ2LfsaJLzSc4m2bdWhUuShjdMz/6jwH2L2o4AU1W1C3i2myfJbuAB\nYHe3zRNJ/PQgSSM2MIir6s+Ary1q3g9MdtOTwMFu+gBwvKquVNUscAHYszqlSpKWa7m97omqmu+m\n54GJbnorMNe33hywbZmvIUlaJSs+xVJVxdJjB8dhXKEkNW254+znk2yuqktJtgCXu/aLwI6+9bZ3\nba+R5Fjf7HRVTS+zFkm6TU13PyuXXsd8wErJ3cDTVfUD3fzjwFeq6oNJjgAbq+pId4H2KXrn6bcB\nzwDfV4teJElVlXfxsHAH7Th8+Amjr2McaoDxqMMarhmHOsahBoCw3Owc2LNPchz4KeC7k7wM/Drw\nm8CJJA8Cs8D9AFU1k+QEMANcBR5aHPSSpPU3VM9+1V/Unv2/sGc/bjXAeNRhDdeMQx3jUAOspGfv\nGHhJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCw\nl6QGGPaS1ADDXpIasNzHEt4Wet8lL0m3v6bDvmfUee8zXCStPU/jSFIDDHtJasCahH2S+5KcTXI+\nySNr8RqSpOGtetgnuQP4b8B9wG7gZ5O8dbVfR5I0vLXo2e8BLlTVbFVdAf4IOLAGryNJGtJahP02\n4OW++bmuTZI0Imsx9HKosYxJnl6D174Zz4z49SVp3axF2F8EdvTN76DXu1/s3Wvw2jeje/1xGOc+\nDjXAeNQxDjXAeNRhDdeMQx3jUMPypWp1bypKcifwN8BPA18AngN+tqpeWtUXkiQNbdV79lV1Nckv\nAZ8A7gA+bNBL0mites9ekjR+1v0O2lZvuEqyI8mnkryY5PNJ3te1b0oyleRcktNJNo661vWS5I4k\nZxYu1rd6LJJsTPKxJC8lmUnyww0fi6Pd78gLSZ5K8h2tHIskH0kyn+SFvrYbvvfuWJ3v8nTfoP2v\na9g3fsPVFeBXqur7gR8BfrF770eAqaraBTzbzbfiYWCGayO4Wj0Wvwt8vKreCvwgcJYGj0WSu4Ff\nAN5eVT9A7zTwIdo5Fh+ll439rvvek+wGHqCXo/cBTyRZMs/Xu2ff7A1XVXWpqp7vpr8NvETv/oP9\nwGS32iRwcDQVrq8k24F3Ab/PtWEOzR2LJG8GfrKqPgK9a15V9Q0aPBbAN+l1iu7qBnrcRW+QRxPH\noqr+DPjaouYbvfcDwPGqulJVs8AFevl6Q+sd9t5wxb/0YN4GfBqYqKr5btE8MDGistbb7wAfAF7p\na2vxWOwEvpTko0k+l+T3kryRBo9FVX0V+C3g7+iF/NeraooGj0WfG733rbx6SPvALF3vsG/+anCS\nNwF/DDxcVd/qX1a9q+W3/TFK8m7gclWd4QaDl1s5FvRGxL0deKKq3g78PYtOU7RyLJJ8L/B+4G56\nYfamJD/Xv04rx+J6hnjvSx6X9Q77YW+4ui0leT29oH+yqk52zfNJNnfLtwCXR1XfOvoxYH+S/wcc\nB/5Dkidp81jMAXNV9Zlu/mP0wv9Sg8fiHcBfVNVXquoq8CfAj9LmsVhwo9+JxVm6vWu7ofUO+88C\n9yS5O8kGehcYTq1zDSORJMCHgZmq+lDfolPA4W76MHBy8ba3m6p6tKp2VNVOehfgPllVP0+bx+IS\n8HKSXV3TvcCLwNM0dizoXZj+kSRv6H5f7qV3Ab/FY7HgRr8Tp4BDSTYk2QncQ+8G1hurqnX9AX6G\n3h22F4Cj6/36o/oBfoLe+enngTPdz33AJnrf03MOOA1sHHWt63xcfgo41U03eSyAHwI+A/wVvd7s\nmxs+Fr9K74/dC/QuSL6+lWNB71PuF4B/pndt8z1LvXfg0S5HzwL/adD+valKkhrgYwklqQGGvSQ1\nwLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDfj/F1Fc2gPCGKcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1dbca42a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exercise 1: You can replace 'Age' with another feature name to plot a\n",
    "# different feature.\n",
    "age_idx = csv_header.index('Age')  # Get the index of the 'Age' column.\n",
    "ages = X[:, age_idx]  # Extract the 'Age' column from the array of features.\n",
    "\n",
    "# hist() takes an array of numbers and computes and plots the histogram.\n",
    "# You can pass in keyword arguments like bins (the number of bins) and\n",
    "# range (the range of values the bins cover).\n",
    "n, bins, patches = plt.hist(ages, bins=10, range=(0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As would be expected, the histogram peaks at an older age (75).\n",
    "\n",
    "___\n",
    "**Exercise 1:** Can you plot the histogram of another feature? For example, Weight_mean or GCS_mean. Hint: if you remove the `bins` and `range` keyword arguments to `plt.hist`, it automatically finds a reasonable set of bins for your data.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "\n",
    "`Scikit-learn` uses a common API for all of their machine learning models, which makes it really easy to try different models. Let's start with a simple [logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "**NOTE:** A logistic regression model is *not* used for regression -- it is used for *classification*. Confusing! It's naming comes from the fact that it uses a linear model (similar to linear regression) to map the inputs to outputs. However, it then passes the output through a function (the logit function) which scales the value to be between 0 and 1, and increases the steepness of the curve around the midpoint - the image below plots the logit function. We can then pick a threshold value (for example, 0.5), below which the point is classified as one class, and above which it is classified as the other class. This is how the model is used for classification.\n",
    "\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" width=\"30%\" height=\"30%\"></img>\n",
    "\n",
    "First, we'll split the dataset into a train and test set, with 80% used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# in training set: 2560\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = int(0.8 * len(X))\n",
    "print '# in training set:', TRAIN_SIZE\n",
    "X_train = X[:TRAIN_SIZE]\n",
    "y_train = y[:TRAIN_SIZE]\n",
    "X_test = X[TRAIN_SIZE:]\n",
    "y_test = y[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because logistic regression is based on a linear model, it won't handle missing values represented by -1 very well. Scikit-learn has an [`Imputer`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) class that provides basic strategies for filling in, or *imputing*, missing values. Let's use the strategy of using the mean of the known values to replace the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "# Initialize the Imputer class with stragegy 'mean'.\n",
    "imp = Imputer(missing_values=-1, strategy='mean')\n",
    "# fit_transform() fits the imputer on the data and applies the imputing\n",
    "# transform.\n",
    "X_train_imputed = imp.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are also sensitive to different ranges for input features. For example, if one input feature has a range of -1 to 1 and another has a range of 0 to 100, the feature with the larger range will disproportionately influence the model. To combat this, we scale all the features to **zero mean and unit variance**, using scikit-learn's [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class.\n",
    "\n",
    "Note: We filled in missing values before scaling, otherwise the number used to represent missing values would distort the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  # Initialize the scaler class.\n",
    "# fit_transform() fits the scaler to the data and applies the transform.\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we're ready to train the model. Scikit-learn's models all have a `fit()` function to fit the model to the training data and a `predict()` function to predict outcomes on new input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression()\n",
    "# Train the model with the training data.\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Model\n",
    "Now that we have a model, how do we tell how good it is? Since we have labelled outcomes to compare predictions to, we can quantitatively measure how well the model is doing. This is where we use the test set. Because we only used the training set to develop the model, we can use the test set to see how well the model works on unseen data. With classification, each outcome is either predicted correctly or not, so the measure of success is binary.\n",
    "\n",
    "Models in scikit-learn usually have a `score()` method, which takes in a set of features and their labels. For logistic regression, this method returns the **accuracy** of the model on the input data. Accuracy is the number of outcomes correctly predicted divided by the total number of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85625\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(imp.transform(X_test))\n",
    "print model.score(X_test_scaled, y_test)\n",
    "# Exercise 2: call model.score() on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Exercise 2:** How does the accuracy on the test set compare to the accuracy on the training set? Can you compute the accuracy on the training set?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from before that the proportion of positive outcomes is fairly low (~14%). This means that accuracy is not necessarily a very useful metric of model performance. A model which predicts survival for each patient (the negative outcome) would still be ~86% accurate!\n",
    "\n",
    "Thankfully, there are other metrics for model performance. Before we go into these metrics, let's define a few terms. In binary classification, there is a **positive** outcome and a **negative** outcome. Typically, the positive outcome is the case you'd like to detect (ex. detecting a malignant tumor, or spam email), and the negative outcome is the more normal, generally expected outcome (ex. not a tumor, or is email). In our example, the positive outcome is an in-hospital death. The prediction results can be one of the following:\n",
    "* **true positive**: a correctly predicted positive outcome\n",
    "* ** false positive**: a negative outcome incorrectly predicted as positive\n",
    "* ** true negative**: a correctly predicted negative outcome\n",
    "* ** false negative**: a positive outcome incorrectly predicted as negative\n",
    "\n",
    "Let's now apply these terms to other metrics for model performance:\n",
    "* **recall**, also known as **sensitivity** or **true positive rate**: of the positive outcomes (in-hospital death), how many did we predict correctly. This is the number of true positives over the total number of positives.\n",
    "* **precision**, also known as **positive predictivity**: of the outcomes predicted as positive, what fraction was correct. This is the number of true positives over the total number of predicted positives.\n",
    "\n",
    "One method of visualizing these metrics is to plot the precision-recall curve. Recall from the logistic regression description above that we set a *threshold* for classifying an outcome as positive or negative. Most models output a prediction *probability* for each data point so you can manually adjust the threshold. For example, if we set the threshold to 0, all outcomes will be predicted as positive. This gives 100% recall and a precision of 14.5% (the proportion of true positives).\n",
    "\n",
    "In the code below, we use scikit-learn's [`precision_recall_curve()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) to compute the precision recall curve. The function adjusts the threshold and computes the precision/recall at each threshold value. This curve allows you to pick the right precision/recall for your application. For example, in a cancer screening test, you may want very high recall, but are ok with low precision because patients will receive follow-up tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEKCAYAAAAVaT4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlIySvwiIiXz/wFcMBfldPzqIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1db1d6c0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# predict_proba() returns probability estimates for each class as an\n",
    "# N x 2 array, where N is the number of data points and 2 is the number\n",
    "# of classes. The negative outcome is the first column, and the positive\n",
    "# outcome is the second column.\n",
    "predicted_probs = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# We use the probability estimate for the positive outcome in\n",
    "# precision_recall_curve().\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, predicted_probs[:,1])\n",
    "\n",
    "# Plot precision and recall.\n",
    "plt.plot(precision, recall, 's-', lw=1)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Exercise 3:** If we want to predict at least 50% of all in-hospital deaths (recall), what is the minimum percentage of predicted deaths that will be wrong (precision)? What if we wanted a recall of 80%?\n",
    "___\n",
    "**Exercise 4:** If I had 1000 patients and set the threshold for 80% recall, how many in-hospital deaths will be incorrectly predicted (false positive)?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One measure of model performance (and the method used in the Physionet challenge) is the minimum of precision and recall. One would pick the point in the curve that maximizes this minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max min of Precision/Recall 0.505376344086\n"
     ]
    }
   ],
   "source": [
    "# Zip takes multiple lists [a1, a2, a3] and [b1, b2, b3] and returns a new list\n",
    "# with elements at the same index combined: [(a1, b1), (a2, b2), (a3, b3)]\n",
    "both = zip(precision, recall)\n",
    "print 'Max min of Precision/Recall', max([min(r) for r in both])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Exercise 5:** Try using a [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) instead of a logistic regression model. Start with the default parameter settings. A random forest model averages the result of many decision trees trained on random subsets of the data. How do the results compare? Hint: replace `linear_model.LogisticRegression` in the code box at the end of the \"Building a Model\" section with `RandomForestClassifier`.\n",
    "\n",
    "What happens if you increase the number of estimators (the number of trees in the forest)? Hint: set `n_estimators` when creating a `RandomForestClassifier` object.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "How do we decide whether to use the logistic regression model or the random forest classifier? In addition, models themselves often have a few parameters than need to be tuned, for example, the number of decision trees in the random forest. How do we make these decisions?\n",
    "\n",
    "We can try all of the various possibilities and evaluate the solution on the test set. The model that gives the best results is the winner. However, we don't want to use our test set for this selection process because we then won't have any unseen data to make a final evaluation. We need a *third* partition to our dataset set to evaluate the model, referred to as the \"validation set\". Instead of creating a fixed third partition as the validation set, we can use the method of **cross-validation.** This uses different splits of the training set for training and evaluation.\n",
    "\n",
    "Scikit-learn has a library of utilities for cross-validation and performance evaluation in the `sklearn.cross-validation` module. It has several classes which automatically generate different splits of the training set. We will be using the `StratifiedKFold` iterator, which splits the data into *n* folds. *n - 1* folds are used for training, and the *nth* fold is used for test. A stratified K-fold maintains approximately the same percentage of each outcome class in each fold as in the complete set.\n",
    "\n",
    "Let's use the stratified K-fold method to create train/test splits and use the average of the results of the K folds to evaluate our model choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max min of Precision/Recall 0.34328358209\n",
      "Max min of Precision/Recall 0.34693877551\n",
      "Max min of Precision/Recall 0.404255319149\n",
      "Max min of Precision/Recall 0.510638297872\n",
      "Max min of Precision/Recall 0.382978723404\n",
      "Max min of Precision/Recall 0.425531914894\n",
      "Max min of Precision/Recall 0.521739130435\n",
      "Max min of Precision/Recall 0.45652173913\n",
      "Max min of Precision/Recall 0.509803921569\n",
      "Max min of Precision/Recall 0.458333333333\n",
      "Mean value over 10 folds: 0.436002473739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "skf = StratifiedKFold(y, 10)  # 10 = number of folds\n",
    "\n",
    "max_precision_recall_values = []\n",
    "for train, test in skf:\n",
    "    # `train` and `test` are lists of the indices for the train and test\n",
    "    # sets.\n",
    "    X_train = X[train]\n",
    "    y_train = y[train]\n",
    "    X_test = X[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    # Here, we repeat all the preprocessing and model training that we\n",
    "    # went through in the code blocks above.\n",
    "    imp = Imputer(missing_values=-1, strategy='mean')\n",
    "    X_train_imputed = imp.fit_transform(X_train)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    model = linear_model.LogisticRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    X_test_scaled = scaler.transform(imp.transform(X_test))\n",
    "    predicted_probs = model.predict_proba(X_test_scaled)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, predicted_probs[:,1])\n",
    "    both = zip(precision, recall)\n",
    "    max_precision_recall = max([min(r) for r in both])\n",
    "    print 'Max min of Precision/Recall', max([min(r) for r in both])\n",
    "    max_precision_recall_values.append(max_precision_recall)\n",
    "\n",
    "print 'Mean value over 10 folds:', numpy.mean(max_precision_recall_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "**Exercise 6:** Use Stratified K-fold to measure the performance of the random forest classifier.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on the Test Set\n",
    "\n",
    "So far, we haven't touched the test set. This was intentional, so that you have a pristine dataset to test your final model on after we went through all the topics above. Try loading the data on your own from [https://raw.githubusercontent.com/lydiagu/ml-tutorial/master/physionet/test-a.csv](https://raw.githubusercontent.com/lydiagu/ml-tutorial/master/physionet/test-a.csv) and evaluating your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Raw data is not a nice csv. How did we preprocess the data?\n",
    "\n",
    "We only used the training set from the challenge website because that's the only labelled dataset available to the public. We further split the challenge training set into a train and test set for this tutorial (`train-a.csv` and `test-a.csv`).\n",
    "\n",
    "The featurization code can be found on [github](https://github.secureserver.net/lgu/techfest-ml-tutorial/blob/master/physionet/create_featurized_datasets.py). It both featurizes the dataset and splits it into train and test sets. Each patient record file is read and stored as a dictionary of metric name to list of measurements. Then for each metric name, we compute the min, max, mean, first value, last value and difference between first and last values. If no measurements were recorded for that metric, all of those features would be -1. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
